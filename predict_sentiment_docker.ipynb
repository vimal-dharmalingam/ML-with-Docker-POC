{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vimald\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vimald\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vimald\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "<ipython-input-1-526a7a1a6f24>:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df['actual_rating'] = test_data['reviews.rating']\n",
      "<ipython-input-1-526a7a1a6f24>:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df['Prediction'] = predicted_result\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************Predcition Done successfully and waiting for the next file\n",
      "*************Waiting for the next file to predict*********************\n",
      "*************Waiting for the next file to predict*********************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-526a7a1a6f24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import boto3\n",
    "import pickle \n",
    "from datetime import timedelta, datetime\n",
    "import os\n",
    "import sys\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# text cleansing libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# downloading required word clouds\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "var_dict={\n",
    "\n",
    "    \"result_file_location\" : \"ml/analytical-result-store\",\n",
    "    \"result_file_name\"    : \"amazon_sentiment_analysis_prediction_pynb_result.csv\",\n",
    "    \"pretrained_model_loc\" : \"ml/prediction_model/amazon_sentiment_analysis_prediction_model.pkl\",\n",
    "    \"inference_data_folder\":\"ml/prediction-data\",\n",
    "    \"inference_file_name\":\"test_data_sentiment_analysis.csv\",\n",
    "    \"s3_bucket_name\" : \"swire-datalake-dev-bucket\",\n",
    "     \"inference_file_backup_path\" : \"ml/prediction-data-backup\",\n",
    "    \"inference_file_backup_key\" : \"test_data_sentiment_analysis.csv\"\n",
    "    #\"aws_id\": \"AKIA4EEJ3XXWKQ37XN2J\",\n",
    "    #\"aws_secret_key\" :\"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\"\n",
    "    }\n",
    "\n",
    "\n",
    "def read_csv_file(bucket_name,inference_data_key): #,aws_access_id,aws_access_key\n",
    "    \"\"\" Read the csv predcition file from s3 using boto3 client\n",
    "    \"\"\"\n",
    "    client = boto3.client('s3') #, aws_access_key_id = aws_access_id , aws_secret_access_key= aws_access_key\n",
    "\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=inference_data_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "\n",
    "    Raw_data = pd.read_csv(StringIO(csv_string))\n",
    "    \n",
    "    return Raw_data\n",
    "\n",
    "\n",
    "def load_pickle_data(bucket_name, model_key): # ,aws_access_id,aws_access_key\n",
    "    \"\"\"\n",
    "    Get the stored pretrained model from S3 bucket\n",
    "    \"\"\"\n",
    "    client = boto3.client('s3') #, aws_access_key_id = aws_access_id, aws_secret_access_key=aws_access_key\n",
    "    response = client.get_object(Bucket=bucket_name, Key=model_key)\n",
    "    body = response['Body'].read()\n",
    "    trained_model = pickle.loads(body)\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "def data_cleansing(Raw_data,trained_tfidf_vector): \n",
    "    \n",
    "    test_data=Raw_data.drop('Unnamed: 0',axis=1)\n",
    "    #lower case all text\n",
    "    test_data[\"reviews.text\"]=test_data[\"reviews.text\"].str.lower() \n",
    "\n",
    "    #tokenization of words\n",
    "    test_data['reviews.text'] = test_data.apply(lambda row: word_tokenize(row['reviews.text']), axis=1) \n",
    "\n",
    "    #only alphanumerical values\n",
    "    test_data[\"reviews.text\"] = test_data['reviews.text'].apply(lambda x: [item for item in x if item.isalpha()]) \n",
    "\n",
    "    #lemmatazing words\n",
    "    test_data['reviews.text'] = test_data['reviews.text'].apply(lambda x : [WordNetLemmatizer().lemmatize(y) for y in x])\n",
    "\n",
    "    #removing useless words\n",
    "    stop = stopwords.words('english')\n",
    "    test_data['reviews.text'] = test_data['reviews.text'].apply(lambda x: [item for item in x if item not in stop])\n",
    "    test_data[\"reviews.text\"] = test_data[\"reviews.text\"].apply(lambda x: str(' '.join(x))) #joining all tokens\n",
    "    sentiment = {1: 0,\n",
    "            2: 0,\n",
    "            3: 0,\n",
    "            4: 1,\n",
    "            5: 1}\n",
    "\n",
    "    test_data[\"sentiment\"] = test_data[\"reviews.rating\"].map(sentiment)\n",
    "    vectorizer =TfidfVectorizer(max_df=0.9)\n",
    "    text = vectorizer.fit_transform(test_data[\"reviews.text\"])\n",
    "    \n",
    "    converted_vector = trained_tfidf_vector.transform(test_data[\"reviews.text\"])  \n",
    "    \n",
    "    return converted_vector\n",
    "\n",
    "\n",
    "def make_prediction (converted_vector_for_model,test_data,trained_model_rf) :\n",
    "    \n",
    "    predicted_result = trained_model_rf[0].predict(converted_vector_for_model)\n",
    "    predicted_proba = trained_model_rf[0].predict_proba(converted_vector_for_model)\n",
    "    result_df = test_data[['reviews.text']]\n",
    "    result_df['actual_rating'] = test_data['reviews.rating']\n",
    "    result_df['Prediction'] = predicted_result\n",
    "    result_df['Probability']=np.round(pd.DataFrame(predicted_proba)[1],2)\n",
    "    sentiment = {0: 'Not satisfied',\n",
    "            1: \"satisfied\"}\n",
    "\n",
    "    result_df[\"sentiment\"] = result_df[\"Prediction\"].map(sentiment)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def write_to_s3(bucket_name,result_key, raw_data): # ,aws_access_id,aws_access_key\n",
    "    csv_buffer = StringIO()\n",
    "    raw_data.to_csv(csv_buffer)\n",
    "    resource = boto3.resource('s3') # , aws_access_key_id= aws_access_id ,aws_secret_access_key=aws_access_key\n",
    "    \n",
    "    return resource.Object(bucket_name,result_key).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "\n",
    "def copy_and_delete_prediction_data():\n",
    "    try:\n",
    "        #backup the prediction data\n",
    "        s3 = boto3.resource('s3')\n",
    "        copy_source = {\n",
    "                       'Bucket' : bucket_name,\n",
    "                     'Key'      : inference_data_key\n",
    "                       }\n",
    "        s3.meta.client.copy(copy_source,bucket_name,inf_backup_key)\n",
    "        # deleting the object\n",
    "        s3_client=boto3.client('s3')\n",
    "        response =s3_client.delete_object(\n",
    "                                     Bucket=bucket_name,\n",
    "                                     Key=inference_data_key\n",
    "                                            )\n",
    "    except ClientError as e:\n",
    "        error_code = e.response[\"Error\"][\"Code\"]\n",
    "        print(\"File not found : \",  e)\n",
    "        \n",
    "        \n",
    "#s3_conn_id = var_dict[\"s3_conn_id\"]\n",
    "result_file_location = var_dict[\"result_file_location\"]\n",
    "result_file_name    =  var_dict[\"result_file_name\"]\n",
    "pretrained_model_loc = var_dict[\"pretrained_model_loc\"]\n",
    "bucket_name = var_dict[\"s3_bucket_name\"]\n",
    "inference_data_folder = var_dict[\"inference_data_folder\"]\n",
    "inference_file_name = var_dict[\"inference_file_name\"]\n",
    "backup_path_inf_file = var_dict[\"inference_file_backup_path\"]\n",
    "backup_filename_inf = var_dict[\"inference_file_backup_key\"]\n",
    "bucket_name=var_dict['s3_bucket_name']  \n",
    "\n",
    "#aws_access_id= var_dict['aws_id']\n",
    "#aws_access_key= var_dict['aws_secret_key']\n",
    "\n",
    "inference_data_key = inference_data_folder + \"/\" + inference_file_name\n",
    "model_key = pretrained_model_loc\n",
    "result_key = result_file_location +\"/\" + result_file_name\n",
    "inf_backup_key = backup_path_inf_file + \"/\"+ backup_filename_inf\n",
    "\n",
    "trained_tfidf_vector = pd.read_pickle(r'trained_tfidf_vector.pkl')\n",
    "\n",
    "\n",
    "\n",
    "def result_func ():\n",
    "    \n",
    "    Raw_data = read_csv_file(bucket_name,inference_data_key)\n",
    "    \n",
    "    trained_model_rf = load_pickle_data(bucket_name, model_key)\n",
    "    \n",
    "    converted_vector = data_cleansing(Raw_data,trained_tfidf_vector)\n",
    "    \n",
    "    result_data = make_prediction (converted_vector,Raw_data,trained_model_rf)\n",
    "    #result_df = result_data.head(10)\n",
    "    #print(result_df)\n",
    "    \n",
    "    write_to_s3(bucket_name,result_key, result_data)\n",
    "    \n",
    "    copy_and_delete_prediction_data()\n",
    "    print(\"****************Predcition Done successfully and waiting for the next file\")\n",
    "    return result_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prediction():\n",
    "    \n",
    "    try:\n",
    "        s3 = boto3.resource('s3')\n",
    "        s3.Object(bucket_name,inference_data_key).load()\n",
    "        #print(time.time())\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(\"*************Waiting for the next file to predict*********************\")\n",
    "\n",
    "    else:\n",
    "        # The object does exist.\n",
    "        result_func ()\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    \n",
    "    while True:\n",
    "        time.sleep(5)\n",
    "        prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
