{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/aherparesh/sentiment-analysis-on-amazon-product-rnn-97-acc\n",
    "# https://www.kaggle.com/lele1995/amazon-reviews-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vimald\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vimald\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vimald\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import boto3\n",
    "import io\n",
    "import time, datetime\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37722</th>\n",
       "      <td>AVqkIdZiv8e3D1O-leaJ</td>\n",
       "      <td>The battery is having more and more trouble ho...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37723</th>\n",
       "      <td>AVqkIdZiv8e3D1O-leaJ</td>\n",
       "      <td>My daughter has had this tablet for almost 2 m...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37724</th>\n",
       "      <td>AVqkIdZiv8e3D1O-leaJ</td>\n",
       "      <td>Very cheap and was not impressed at all never ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37725</th>\n",
       "      <td>AVqkIdZiv8e3D1O-leaJ</td>\n",
       "      <td>Hard to use, Lots of ads, and Randomly closes ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37726</th>\n",
       "      <td>AVqkIdZiv8e3D1O-leaJ</td>\n",
       "      <td>I wish it has some more of the apps from the p...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37727 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0      AVqkIhwDv8e3D1O-lebb   \n",
       "1      AVqkIhwDv8e3D1O-lebb   \n",
       "2      AVqkIhwDv8e3D1O-lebb   \n",
       "3      AVqkIhwDv8e3D1O-lebb   \n",
       "4      AVqkIhwDv8e3D1O-lebb   \n",
       "...                     ...   \n",
       "37722  AVqkIdZiv8e3D1O-leaJ   \n",
       "37723  AVqkIdZiv8e3D1O-leaJ   \n",
       "37724  AVqkIdZiv8e3D1O-leaJ   \n",
       "37725  AVqkIdZiv8e3D1O-leaJ   \n",
       "37726  AVqkIdZiv8e3D1O-leaJ   \n",
       "\n",
       "                                            reviews.text  reviews.rating  \n",
       "0      This product so far has not disappointed. My c...             5.0  \n",
       "1      great for beginner or experienced person. Boug...             5.0  \n",
       "2      Inexpensive tablet for him to use and learn on...             5.0  \n",
       "3      I've had my Fire HD 8 two weeks now and I love...             4.0  \n",
       "4      I bought this for my grand daughter when she c...             5.0  \n",
       "...                                                  ...             ...  \n",
       "37722  The battery is having more and more trouble ho...             2.0  \n",
       "37723  My daughter has had this tablet for almost 2 m...             3.0  \n",
       "37724  Very cheap and was not impressed at all never ...             1.0  \n",
       "37725  Hard to use, Lots of ads, and Randomly closes ...             2.0  \n",
       "37726  I wish it has some more of the apps from the p...             3.0  \n",
       "\n",
       "[37727 rows x 3 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = 'swire-datalake-dev-bucket'\n",
    "inference_data_key = 'ml/training_data/Amazon_after_eda_for_training.csv'\n",
    "\n",
    "\n",
    "def read_csv_file(bucket_name,inference_data_key): #,aws_access_id,aws_access_key\n",
    "    \"\"\" Read the csv predcition file from s3 using boto3 client\n",
    "    \"\"\"\n",
    "    client = boto3.client('s3') #, aws_access_key_id = aws_access_id , aws_secret_access_key= aws_access_key\n",
    "\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=inference_data_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "\n",
    "    Raw_data = pd.read_csv(StringIO(csv_string))\n",
    "    \n",
    "    return Raw_data\n",
    "from_eda = read_csv_file(bucket,inference_data_key)\n",
    "from_eda =from_eda.drop('Unnamed: 0',axis=1)\n",
    "from_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_eda = shuffle(from_eda)\n",
    "from_eda = shuffle(from_eda)\n",
    "from_eda = shuffle(from_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = from_eda.iloc[:30000,:]\n",
    "test_data =  from_eda.iloc[30000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    4938\n",
       "4.0    1754\n",
       "3.0     556\n",
       "1.0     273\n",
       "2.0     206\n",
       "Name: reviews.rating, dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['reviews.rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for testing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '48A2D76F600029B1',\n",
       "  'HostId': '2hV4Pf2BWGireVGeWWvMZqsOZQpA2zB6oK+TYpV6NGaxIifSm6z673yJ0Mj4wkewfIsuwdXV1lY=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '2hV4Pf2BWGireVGeWWvMZqsOZQpA2zB6oK+TYpV6NGaxIifSm6z673yJ0Mj4wkewfIsuwdXV1lY=',\n",
       "   'x-amz-request-id': '48A2D76F600029B1',\n",
       "   'date': 'Wed, 30 Sep 2020 09:27:36 GMT',\n",
       "   'x-amz-version-id': 'w1bgzy5LWduBXkFFN6adg2dYJriGWlBM',\n",
       "   'etag': '\"1a5a15ca1a66ffa0f71b499e496e3e98\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 1},\n",
       " 'ETag': '\"1a5a15ca1a66ffa0f71b499e496e3e98\"',\n",
       " 'VersionId': 'w1bgzy5LWduBXkFFN6adg2dYJriGWlBM'}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ml/training_data/Amazon_after_eda_for_training.csv'\n",
    "def write_to_s3(bucket_name,result_key, raw_data): # ,aws_access_id,aws_access_key\n",
    "    csv_buffer = StringIO()\n",
    "    raw_data.to_csv(csv_buffer)\n",
    "    resource = boto3.resource('s3') # , aws_access_key_id= aws_access_id ,aws_secret_access_key=aws_access_key\n",
    "    \n",
    "    return resource.Object(bucket_name,result_key).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "\n",
    "training_data_key = 'ml/training_data/training_data_sentiment_analysis.csv'\n",
    "test_data_key = 'ml/training_data/test_data_sentiment_analysis.csv'\n",
    "\n",
    "write_to_s3(bucket,training_data_key, training_data)\n",
    "write_to_s3(bucket,test_data_key, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#training_data.to_csv('training_data_sentiment_analysis.csv')\n",
    "#test_data.to_csv('test_data_sentiment_analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVphgVaX1cnluZ0-DR74</td>\n",
       "      <td>Couldn't ask for much more, performs smoothly,...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVpfl8cLLJeJML43AE3S</td>\n",
       "      <td>Ask Alexa ANYTHING and she'll search the web. ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVpfl8cLLJeJML43AE3S</td>\n",
       "      <td>Having the echo is great, especially for playi...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVphgVaX1cnluZ0-DR74</td>\n",
       "      <td>Bought this tab for the $40 dollar sale price ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVphgVaX1cnluZ0-DR74</td>\n",
       "      <td>This tablet is a great back up tablet for the ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       reviews.text  \\\n",
       "0  AVphgVaX1cnluZ0-DR74  Couldn't ask for much more, performs smoothly,...   \n",
       "1  AVpfl8cLLJeJML43AE3S  Ask Alexa ANYTHING and she'll search the web. ...   \n",
       "2  AVpfl8cLLJeJML43AE3S  Having the echo is great, especially for playi...   \n",
       "3  AVphgVaX1cnluZ0-DR74  Bought this tab for the $40 dollar sale price ...   \n",
       "4  AVphgVaX1cnluZ0-DR74  This tablet is a great back up tablet for the ...   \n",
       "\n",
       "   reviews.rating  \n",
       "0             4.0  \n",
       "1             5.0  \n",
       "2             5.0  \n",
       "3             3.0  \n",
       "4             5.0  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_data_key = 'ml/training_data/training_data_sentiment_analysis.csv'\n",
    "def read_csv_file(bucket_name,inference_data_key): #,aws_access_id,aws_access_key\n",
    "    \"\"\" Read the csv predcition file from s3 using boto3 client\n",
    "    \"\"\"\n",
    "    client = boto3.client('s3') #, aws_access_key_id = aws_access_id , aws_secret_access_key= aws_access_key\n",
    "\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=inference_data_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "\n",
    "    Raw_data = pd.read_csv(StringIO(csv_string))\n",
    "    \n",
    "    return Raw_data\n",
    "final = read_csv_file(bucket,inference_data_key)\n",
    "final = final.drop('Unnamed: 0',axis=1)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower case all text\n",
    "final[\"reviews.text\"]=final[\"reviews.text\"].str.lower() \n",
    "\n",
    "#tokenization of words\n",
    "final['reviews.text'] = final.apply(lambda row: word_tokenize(row['reviews.text']), axis=1) \n",
    "\n",
    "#only alphanumerical values\n",
    "final[\"reviews.text\"] = final['reviews.text'].apply(lambda x: [item for item in x if item.isalpha()]) \n",
    "\n",
    "#lemmatazing words\n",
    "final['reviews.text'] = final['reviews.text'].apply(lambda x : [WordNetLemmatizer().lemmatize(y) for y in x])\n",
    "\n",
    "#removing useless words\n",
    "stop = stopwords.words('english')\n",
    "final['reviews.text'] = final['reviews.text'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[\"reviews.text\"] = final[\"reviews.text\"].apply(lambda x: str(' '.join(x))) #joining all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVphgVaX1cnluZ0-DR74</td>\n",
       "      <td>could ask much performs smoothly expandable me...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVpfl8cLLJeJML43AE3S</td>\n",
       "      <td>ask alexa anything search web love able contro...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVpfl8cLLJeJML43AE3S</td>\n",
       "      <td>echo great especially playing music searching ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVphgVaX1cnluZ0-DR74</td>\n",
       "      <td>bought tab dollar sale price nice little syste...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVphgVaX1cnluZ0-DR74</td>\n",
       "      <td>tablet great back tablet money</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       reviews.text  \\\n",
       "0  AVphgVaX1cnluZ0-DR74  could ask much performs smoothly expandable me...   \n",
       "1  AVpfl8cLLJeJML43AE3S  ask alexa anything search web love able contro...   \n",
       "2  AVpfl8cLLJeJML43AE3S  echo great especially playing music searching ...   \n",
       "3  AVphgVaX1cnluZ0-DR74  bought tab dollar sale price nice little syste...   \n",
       "4  AVphgVaX1cnluZ0-DR74                     tablet great back tablet money   \n",
       "\n",
       "   reviews.rating  \n",
       "0             4.0  \n",
       "1             5.0  \n",
       "2             5.0  \n",
       "3             3.0  \n",
       "4             5.0  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = {1: 0,\n",
    "            2: 0,\n",
    "            3: 0,\n",
    "            4: 1,\n",
    "            5: 1}\n",
    "\n",
    "final[\"sentiment\"] = final[\"reviews.rating\"].map(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4377, 25623)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final[final[\"sentiment\"]==0]),len(final[final[\"sentiment\"]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/15361885/linking-text-feature-names-to-their-tfidf-value\n",
    "# building tfidf matrix to train models \n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# vectorizer =TfidfVectorizer(max_df=0.9)\n",
    "# text = vectorizer.fit_transform(final[\"reviews.text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.9)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tf-idf based vectors\n",
    "tf = TfidfVectorizer(max_df=0.9)\n",
    "\n",
    "# Fit the model\n",
    "tf_transformer = tf.fit(final[\"reviews.text\"])\n",
    "\n",
    "# Dump the file\n",
    "pickle.dump(tf_transformer, open(\"trained_tfidf_vector.pkl\", \"wb\"))\n",
    "tf_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30000x11126 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 420812 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_new =  tf_transformer.transform(final[\"reviews.text\"])\n",
    "text_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = dict(zip(vectorizer.get_feature_names(), text.data))\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_new, final[\"sentiment\"], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try logistic regression first\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr_classifier = LogisticRegression(random_state=1)\n",
    "lgr_classifier.fit(x_train, y_train)\n",
    "y_pred = lgr_classifier.predict(x_test)\n",
    "y_pred_tr = lgr_classifier.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.9097777777777778\n",
      "Train accuracy 0.9271428571428572\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy', sum(y_test == y_pred)/len(y_test))\n",
    "print('Train accuracy', sum(y_train == y_pred_tr)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report(Train)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.55      0.69      3048\n",
      "           1       0.93      0.99      0.96     17952\n",
      "\n",
      "    accuracy                           0.93     21000\n",
      "   macro avg       0.92      0.77      0.82     21000\n",
      "weighted avg       0.93      0.93      0.92     21000\n",
      "\n",
      "Classification Report(Test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.47      0.61      1329\n",
      "           1       0.91      0.99      0.95      7671\n",
      "\n",
      "    accuracy                           0.91      9000\n",
      "   macro avg       0.88      0.73      0.78      9000\n",
      "weighted avg       0.91      0.91      0.90      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report(Train)\")\n",
    "print(classification_report(y_train, y_pred_tr))\n",
    "print(\"Classification Report(Test)\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "y_pred_tr = classifier.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.9301111111111111\n",
      "Train accuracy 0.9995238095238095\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy', sum(y_test == y_pred)/len(y_test))\n",
    "print('Train accuracy', sum(y_train == y_pred_tr)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report(Train)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3048\n",
      "           1       1.00      1.00      1.00     17952\n",
      "\n",
      "    accuracy                           1.00     21000\n",
      "   macro avg       1.00      1.00      1.00     21000\n",
      "weighted avg       1.00      1.00      1.00     21000\n",
      "\n",
      "Classification Report(Test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.58      0.71      1329\n",
      "           1       0.93      0.99      0.96      7671\n",
      "\n",
      "    accuracy                           0.93      9000\n",
      "   macro avg       0.92      0.79      0.84      9000\n",
      "weighted avg       0.93      0.93      0.92      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report(Train)\")\n",
    "print(classification_report(y_train, y_pred_tr))\n",
    "print(\"Classification Report(Test)\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "import pickle\n",
    "filename = 'trained_model_rf_sentiment_analysis_10872_features.pkl'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tag_time = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "time = datetime.datetime.now().strftime(\"%Y-%m-%d#%H:%M:%S\")\n",
    "tags = 'Training_version={}&Trained_date={}'.format('V1', Tag_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_amazon_senti-analysis_model_rf_2020-09-30#09:56:16.pkl\n",
      "V1_amazon_senti-analysis_model_rf_2020-09-30#09:56:16.pkl\n",
      "Enabled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '5E48DAB992A627A1',\n",
       "  'HostId': 'TxdYXCDJgeE16nn2OFPjjdeoU9OHBixn8SB51AQzB3VHA0PyzGGBc35jN1g01MQ/UZXg90GGSL4=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'TxdYXCDJgeE16nn2OFPjjdeoU9OHBixn8SB51AQzB3VHA0PyzGGBc35jN1g01MQ/UZXg90GGSL4=',\n",
       "   'x-amz-request-id': '5E48DAB992A627A1',\n",
       "   'date': 'Wed, 30 Sep 2020 09:56:19 GMT',\n",
       "   'x-amz-version-id': 'yTJKFoJkvywXuygPI0DphVz7.269hMgo',\n",
       "   'etag': '\"d21fc5623517d6799a9a02341420b176\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 1},\n",
       " 'ETag': '\"d21fc5623517d6799a9a02341420b176\"',\n",
       " 'VersionId': 'yTJKFoJkvywXuygPI0DphVz7.269hMgo'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## arguments \n",
    "\n",
    "model_name = 'V1_amazon_senti-analysis_model_rf_'+time+'.pkl'\n",
    "print(model_name)\n",
    "path = 'ml/training_models/'\n",
    "\n",
    "bucket = 'swire-datalake-dev-bucket'\n",
    "trained_model =  classifier\n",
    "key = path + model_name\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "def save_trained_model_to_s3 (trained_model, bucket, key):\n",
    "    \n",
    "    \"\"\"\n",
    "    store the saved modelas pickle file with the tagging \n",
    "    \"\"\"    \n",
    "    pickle_byte_obj = pickle.dumps([trained_model]) \n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    versioning = s3_resource.BucketVersioning(bucket)\n",
    "    # check status\n",
    "    print(versioning.status)\n",
    "    # enable versioning\n",
    "    versioning.enable()\n",
    "    response = s3_client.put_object(\n",
    "                                    Bucket=bucket,\n",
    "                                    Body=pickle_byte_obj,\n",
    "                                    Key=key,\n",
    "                                    Tagging=tags\n",
    "                                    )\n",
    "    \n",
    "    return response\n",
    "save_trained_model_to_s3 (trained_model, bucket, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save trained vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_amazon_senti-analysis_model_rf_2020-09-30#09:56:16.pkl\n",
      "V1_amazon_senti-analysis_model_rf_2020-09-30#09:56:16.pkl\n",
      "Enabled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'FQDV9NCMDT7W2Z7M',\n",
       "  'HostId': 'ILGlf23+wGNBcr/P2+jOhJ18iUMxCH/UzJiSRlhcdFetshvS2AMMz3NdFRodLyB7EjYplKE79dM=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'ILGlf23+wGNBcr/P2+jOhJ18iUMxCH/UzJiSRlhcdFetshvS2AMMz3NdFRodLyB7EjYplKE79dM=',\n",
       "   'x-amz-request-id': 'FQDV9NCMDT7W2Z7M',\n",
       "   'date': 'Wed, 30 Sep 2020 09:59:31 GMT',\n",
       "   'x-amz-version-id': 'jZAhDWAu6t5DHtPz7VM_zj6z.ZS_yWo6',\n",
       "   'etag': '\"d21fc5623517d6799a9a02341420b176\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 1},\n",
       " 'ETag': '\"d21fc5623517d6799a9a02341420b176\"',\n",
       " 'VersionId': 'jZAhDWAu6t5DHtPz7VM_zj6z.ZS_yWo6'}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## arguments \n",
    "\n",
    "vector_name = 'V1_tfidf_vector_amazon'+time+'.pkl'\n",
    "print(model_name)\n",
    "path = 'ml/training_models/'\n",
    "\n",
    "bucket = 'swire-datalake-dev-bucket'\n",
    "trained_vector =  tf_transformer\n",
    "key = path + vector_name\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "def save_trained_tfidf_vector_to_s3 (trained_vector, bucket, key):\n",
    "    \n",
    "    \"\"\"\n",
    "    store the saved modelas pickle file with the tagging \n",
    "    \"\"\"    \n",
    "    pickle_byte_obj = pickle.dumps([trained_model]) \n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    versioning = s3_resource.BucketVersioning(bucket)\n",
    "    # check status\n",
    "    print(versioning.status)\n",
    "    # enable versioning\n",
    "    versioning.enable()\n",
    "    response = s3_client.put_object(\n",
    "                                    Bucket=bucket,\n",
    "                                    Body=pickle_byte_obj,\n",
    "                                    Key=key,\n",
    "                                    Tagging=tags\n",
    "                                    )\n",
    "    \n",
    "    return response\n",
    "save_trained_tfidf_vector_to_s3  (trained_vector, bucket, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy updated model to prediction model store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained_model_name == V1_amazon_senti-analysis_model_rf_2020-09-30#09:56:16.pkl\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/44384072/move-copy-data-from-one-folder-to-another-on-aws-s3\n",
    "\n",
    "bucket ='swire-datalake-dev-bucket'\n",
    "#training \n",
    "train_path = \"ml/training_models\"\n",
    "trained_model_file_name = \"V1_amazon_senti-analysis_model_rf_2020-09-30#09:56:16.pkl\"\n",
    "trained_model_key  = train_path + \"/\" +  trained_model_file_name\n",
    "#tags\n",
    "tags = 'Trained_model_name == {}'.format(trained_model_file_name)\n",
    "print(tags)\n",
    "# prediction\n",
    "pre_path = \"ml/prediction_model\"\n",
    "prediction_model_file_name =  \"amazon_sentiment_analysis_prediction_model.pkl\" \n",
    "predic_model_key =  pre_path + \"/\" + prediction_model_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def update_the_prediction_model (bucket,trained_model_key,predic_model_key):\n",
    "    \"\"\" \n",
    "    SAve the updated trained model to prediction storage\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    versioning = s3.BucketVersioning(bucket)\n",
    "    # check status\n",
    "    print(versioning.status)\n",
    "    copy_source =  {\n",
    "                   'Bucket':bucket,\n",
    "                   'Key': trained_model_key\n",
    "                    } \n",
    "    return s3.meta.client.copy(copy_source, bucket, predic_model_key)\n",
    "\n",
    "\n",
    "update_the_prediction_model (bucket,trained_model_key,predic_model_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires\n",
    "x_train = x_train\n",
    "y_train =  y_train\n",
    "classifier_name =classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC-Score: 0.9999823183344563\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "\n",
    "predictions = cross_val_predict(classifier, x_train, y_train, cv=3)\n",
    "confusion_matrix(y_train, predictions)\n",
    "\n",
    "\n",
    "\n",
    "# getting the probabilities of our predictions\n",
    "y_scores = classifier_name.predict_proba(x_train)\n",
    "y_scores = y_scores[:,1]\n",
    "\n",
    "\n",
    "\n",
    "AUC_score = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "f1_score=f1_score(y_train, predictions)\n",
    "\n",
    "Precision=precision_score(y_train, predictions)\n",
    "\n",
    "Recall=recall_score(y_train, predictions)\n",
    "\n",
    "Accuracy= round(classifier_name.score(x_train, y_train) * 100, 2)\n",
    "\n",
    "cm=confusion_matrix(y_train, predictions)\n",
    "\n",
    "r_a_score = roc_auc_score(y_train, y_scores)\n",
    "print(\"ROC-AUC-Score:\", r_a_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"model_name\": \"Random Forest\", \"model_parameters\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"class_weight\": null, \"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"auto\", \"max_leaf_nodes\": null, \"max_samples\": null, \"min_impurity_decrease\": 0.0, \"min_impurity_split\": null, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 100, \"n_jobs\": null, \"oob_score\": false, \"random_state\": null, \"verbose\": 0, \"warm_start\": false}, \"evaluation_metrics\": {\"accuracy\": 99.95, \"cross_val_score\": NaN, \"auc_score\": 100.0, \"f1_score\": 95.85, \"precision\": 92.68, \"recall\": 99.25, \"confusion_matrix\": \"[[ 1640  1408]\\\\n [  135 17817]]\", \"time_stamp\": \"2020-10-01#01:27:23\"}}'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics={\"accuracy\":Accuracy,\n",
    "                    \"cross_val_score\": np.NaN,\n",
    "                   \"auc_score\":round(AUC_score*100,2),\n",
    "                   \"f1_score\":round(f1_score*100,2),\n",
    "                   \"precision\":round(Precision*100,2),\n",
    "                   \"recall\":round(Recall*100,2),\n",
    "                   \"confusion_matrix\": str(cm),\n",
    "                    \"time_stamp\":datetime.datetime.now().strftime(\"%Y-%m-%d#%H:%M:%S\") }\n",
    "\n",
    "\n",
    "model_params = classifier_name.get_params()\n",
    "#metadata=list(x_train.columns)\n",
    "\n",
    "meta = {\n",
    "        \"model_name\":\"Random Forest\" ,\n",
    "        'model_parameters':model_params,\n",
    "        #'model_features':metadata,\n",
    "        \"evaluation_metrics\":evaluation_metrics \n",
    "       }\n",
    "json_meta = json.dumps(meta)\n",
    "json_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '40B5E3CB3AD6890D',\n",
       "  'HostId': 'N+B7uGv3nUEMzgSBafuY3swk7zJgrfEzKVfTls04eFHZgajOCBkeYRie23T+lGi0Wo7tN5AUFmw=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'N+B7uGv3nUEMzgSBafuY3swk7zJgrfEzKVfTls04eFHZgajOCBkeYRie23T+lGi0Wo7tN5AUFmw=',\n",
       "   'x-amz-request-id': '40B5E3CB3AD6890D',\n",
       "   'date': 'Thu, 01 Oct 2020 01:09:39 GMT',\n",
       "   'x-amz-version-id': 'njwavusSioyqdS2yjXd0qptQWxuRyxbp',\n",
       "   'etag': '\"fc25698780a0baf03c65c929fa7d30b0\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 1},\n",
       " 'ETag': '\"fc25698780a0baf03c65c929fa7d30b0\"',\n",
       " 'VersionId': 'njwavusSioyqdS2yjXd0qptQWxuRyxbp'}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket ='swire-datalake-dev-bucket'\n",
    "path = 'ml/metadata/nlp/amazon_sentiment_analysis/model_metadata'\n",
    "file_name = 'V1_Rf_'+time+'.txt'\n",
    "\n",
    "key = path + \"/\" + file_name\n",
    "tags = 'Training_version={}&Trained_date={}'.format('V2', Tag_time)\n",
    "\n",
    "def write_model_metadata_to_s3 (bucket,key,data,tags):  \n",
    "    \"\"\"\n",
    "    store the metadata like hyperparameters, input features and \n",
    "    evaluation metrics with the tagging \n",
    "    \"\"\"\n",
    "    \n",
    "    # Versioning \n",
    "    s3_resource = boto3.resource('s3')\n",
    "    versioning = s3_resource.BucketVersioning(bucket)\n",
    "    versioning.enable()\n",
    "    # Put object\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.put_object(\n",
    "                                    Bucket=bucket,\n",
    "                                    Body=data,\n",
    "                                    Key=key,\n",
    "                                    Tagging=tags\n",
    "                                    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "write_model_metadata_to_s3 (bucket,key,json_meta,tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save multiple model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires\n",
    "x_train = x_train\n",
    "y_train =  y_train\n",
    "classifier_name = lgr_classifier #classifier\n",
    "model_name =\"logistic_regression\" # \"Random Forest\" # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC-Score: 0.9517414713514253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "\n",
    "predictions = cross_val_predict(classifier_name, x_train, y_train, cv=3)\n",
    "confusion_matrix(y_train, predictions)\n",
    "\n",
    "\n",
    "\n",
    "# getting the probabilities of our predictions\n",
    "y_scores = classifier_name.predict_proba(x_train)\n",
    "y_scores = y_scores[:,1]\n",
    "\n",
    "\n",
    "\n",
    "AUC_score = roc_auc_score(y_train, y_scores)\n",
    "\n",
    "f1_score=f1_score(y_train, predictions)\n",
    "\n",
    "Precision=precision_score(y_train, predictions)\n",
    "\n",
    "Recall=recall_score(y_train, predictions)\n",
    "\n",
    "Accuracy= round(classifier_name.score(x_train, y_train) * 100, 2)\n",
    "\n",
    "cm=confusion_matrix(y_train, predictions)\n",
    "\n",
    "r_a_score = roc_auc_score(y_train, y_scores)\n",
    "print(\"ROC-AUC-Score:\", r_a_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics={\"accuracy\":Accuracy,\n",
    "                    \"cross_val_score\": np.NaN,\n",
    "                   \"auc_score\":round(AUC_score*100,2),\n",
    "                   \"f1_score\":round(f1_score*100,2),\n",
    "                   \"precision\":round(Precision*100,2),\n",
    "                   \"recall\":round(Recall*100,2),\n",
    "                   \"confusion_matrix\": str(cm),\n",
    "                    \"time_stamp\":datetime.datetime.now().strftime(\"%Y-%m-%d#%H:%M:%S\") }\n",
    "\n",
    "\n",
    "model_params = classifier_name.get_params()\n",
    "#metadata=list(x_train.columns)\n",
    "\n",
    "meta2 = {\n",
    "        \"model_name\": model_name,\n",
    "        'model_parameters':model_params,\n",
    "        #'model_features':metadata,\n",
    "        \"evaluation_metrics\":evaluation_metrics \n",
    "       }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'Random Forest',\n",
       " 'model_parameters': {'bootstrap': True,\n",
       "  'ccp_alpha': 0.0,\n",
       "  'class_weight': None,\n",
       "  'criterion': 'gini',\n",
       "  'max_depth': None,\n",
       "  'max_features': 'auto',\n",
       "  'max_leaf_nodes': None,\n",
       "  'max_samples': None,\n",
       "  'min_impurity_decrease': 0.0,\n",
       "  'min_impurity_split': None,\n",
       "  'min_samples_leaf': 1,\n",
       "  'min_samples_split': 2,\n",
       "  'min_weight_fraction_leaf': 0.0,\n",
       "  'n_estimators': 100,\n",
       "  'n_jobs': None,\n",
       "  'oob_score': False,\n",
       "  'random_state': None,\n",
       "  'verbose': 0,\n",
       "  'warm_start': False},\n",
       " 'evaluation_metrics': {'accuracy': 99.95,\n",
       "  'cross_val_score': nan,\n",
       "  'auc_score': 100.0,\n",
       "  'f1_score': 95.81,\n",
       "  'precision': 92.66,\n",
       "  'recall': 99.18,\n",
       "  'confusion_matrix': '[[ 1638  1410]\\n [  148 17804]]',\n",
       "  'time_stamp': '2020-10-01#01:49:18'}}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Randomforest_meta': {'model_name': 'Random Forest',\n",
       "  'model_parameters': {'bootstrap': True,\n",
       "   'ccp_alpha': 0.0,\n",
       "   'class_weight': None,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'max_leaf_nodes': None,\n",
       "   'max_samples': None,\n",
       "   'min_impurity_decrease': 0.0,\n",
       "   'min_impurity_split': None,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'min_weight_fraction_leaf': 0.0,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': None,\n",
       "   'oob_score': False,\n",
       "   'random_state': None,\n",
       "   'verbose': 0,\n",
       "   'warm_start': False},\n",
       "  'evaluation_metrics': {'accuracy': 99.95,\n",
       "   'cross_val_score': nan,\n",
       "   'auc_score': 100.0,\n",
       "   'f1_score': 95.81,\n",
       "   'precision': 92.66,\n",
       "   'recall': 99.18,\n",
       "   'confusion_matrix': '[[ 1638  1410]\\n [  148 17804]]',\n",
       "   'time_stamp': '2020-10-01#01:49:18'}},\n",
       " 'logistic_regression_meta': {'model_name': 'logistic_regression',\n",
       "  'model_parameters': {'C': 1.0,\n",
       "   'class_weight': None,\n",
       "   'dual': False,\n",
       "   'fit_intercept': True,\n",
       "   'intercept_scaling': 1,\n",
       "   'l1_ratio': None,\n",
       "   'max_iter': 100,\n",
       "   'multi_class': 'auto',\n",
       "   'n_jobs': None,\n",
       "   'penalty': 'l2',\n",
       "   'random_state': 1,\n",
       "   'solver': 'lbfgs',\n",
       "   'tol': 0.0001,\n",
       "   'verbose': 0,\n",
       "   'warm_start': False},\n",
       "  'evaluation_metrics': {'accuracy': 92.71,\n",
       "   'cross_val_score': nan,\n",
       "   'auc_score': 95.17,\n",
       "   'f1_score': 95.01,\n",
       "   'precision': 91.37,\n",
       "   'recall': 98.96,\n",
       "   'confusion_matrix': '[[ 1369  1679]\\n [  186 17766]]',\n",
       "   'time_stamp': '2020-10-01#01:52:19'}}}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the names for each model\n",
    "#meta_name1 = model_name+\"_meta\" \n",
    "meta_name2 = model_name+\"_meta\" \n",
    "#meta2\n",
    "meta={\"Randomforest_meta\" : meta1,\n",
    "      meta_name2 : meta2\n",
    "      }\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Randomforest_meta\": {\"model_name\": \"Random Forest\", \"model_parameters\": {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"class_weight\": null, \"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"auto\", \"max_leaf_nodes\": null, \"max_samples\": null, \"min_impurity_decrease\": 0.0, \"min_impurity_split\": null, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 100, \"n_jobs\": null, \"oob_score\": false, \"random_state\": null, \"verbose\": 0, \"warm_start\": false}, \"evaluation_metrics\": {\"accuracy\": 99.95, \"cross_val_score\": NaN, \"auc_score\": 100.0, \"f1_score\": 95.81, \"precision\": 92.66, \"recall\": 99.18, \"confusion_matrix\": \"[[ 1638  1410]\\\\n [  148 17804]]\", \"time_stamp\": \"2020-10-01#01:49:18\"}}, \"logistic_regression_meta\": {\"model_name\": \"logistic_regression\", \"model_parameters\": {\"C\": 1.0, \"class_weight\": null, \"dual\": false, \"fit_intercept\": true, \"intercept_scaling\": 1, \"l1_ratio\": null, \"max_iter\": 100, \"multi_class\": \"auto\", \"n_jobs\": null, \"penalty\": \"l2\", \"random_state\": 1, \"solver\": \"lbfgs\", \"tol\": 0.0001, \"verbose\": 0, \"warm_start\": false}, \"evaluation_metrics\": {\"accuracy\": 92.71, \"cross_val_score\": NaN, \"auc_score\": 95.17, \"f1_score\": 95.01, \"precision\": 91.37, \"recall\": 98.96, \"confusion_matrix\": \"[[ 1369  1679]\\\\n [  186 17766]]\", \"time_stamp\": \"2020-10-01#01:52:19\"}}}'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_meta = json.dumps(meta)\n",
    "json_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket ='swire-datalake-dev-bucket'\n",
    "path = 'ml/metadata/nlp/amazon_sentiment_analysis/multiple_model_results'\n",
    "file_name = 'V1__'+time+'.txt'\n",
    "\n",
    "key = path + \"/\" + file_name\n",
    "tags = 'Training_version={}&Trained_date={}'.format('V1', Tag_time)\n",
    "\n",
    "def write_model_metadata_to_s3 (bucket,key,data,tags):  \n",
    "    \"\"\"\n",
    "    store the metadata like hyperparameters, input features and \n",
    "    evaluation metrics with the tagging \n",
    "    \"\"\"\n",
    "    \n",
    "    # Versioning \n",
    "    s3_resource = boto3.resource('s3')\n",
    "    versioning = s3_resource.BucketVersioning(bucket)\n",
    "    versioning.enable()\n",
    "    # Put object\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.put_object(\n",
    "                                    Bucket=bucket,\n",
    "                                    Body=data,\n",
    "                                    Key=key,\n",
    "                                    Tagging=tags\n",
    "                                    )\n",
    "    \n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '3B5C832D0E3E967E',\n",
       "  'HostId': 'smMh4nn+G6aTFOckTRpLHdBfJWVuOAZ3NWWqLSLtDmvNXu1CLS2Owv1KhHofLIvZoHglysuY24o=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'smMh4nn+G6aTFOckTRpLHdBfJWVuOAZ3NWWqLSLtDmvNXu1CLS2Owv1KhHofLIvZoHglysuY24o=',\n",
       "   'x-amz-request-id': '3B5C832D0E3E967E',\n",
       "   'date': 'Thu, 01 Oct 2020 01:56:45 GMT',\n",
       "   'x-amz-version-id': 'dqnpRxCCmys0PDeEXeqU7fg1XlJk0J_C',\n",
       "   'etag': '\"a56abfc37bb81f3c4f75b47f9f239070\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 1},\n",
       " 'ETag': '\"a56abfc37bb81f3c4f75b47f9f239070\"',\n",
       " 'VersionId': 'dqnpRxCCmys0PDeEXeqU7fg1XlJk0J_C'}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_model_metadata_to_s3 (bucket,key,json_meta,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to improve performance __-===============++++++++++++++++++++++++++++++++++++++++++++++++++=================-_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# parameters = {\"n_estimators\": [10,50],\n",
    "#              \"criterion\":(\"gini\",\"entropy\")}\n",
    "# classifier = RandomForestClassifier()\n",
    "# clf = GridSearchCV(classifier, parameters, cv=5)\n",
    "# clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter: {'criterion': 'entropy', 'n_estimators': 50}\n",
      "Best accuracy: 0.9350198510502633\n"
     ]
    }
   ],
   "source": [
    "#Viewing best parameters in Grid Search\n",
    "best_parameter = clf.best_params_\n",
    "best_accuracy = clf.best_score_ #best cros validated mean\n",
    "print('Best parameter: ' + str(best_parameter))\n",
    "print('Best accuracy: ' + str(best_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.9276666666666666\n",
      "Train accuracy 0.9996190476190476\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(criterion = best_parameter[\"criterion\"], \n",
    "                                    n_estimators = best_parameter[\"n_estimators\"])\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "y_pred_tr = classifier.predict(x_train)\n",
    "print('Test accuracy', sum(y_test == y_pred)/len(y_test))\n",
    "print('Train accuracy', sum(y_train == y_pred_tr)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report(Train)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1459\n",
      "           1       1.00      1.00      1.00     19541\n",
      "\n",
      "    accuracy                           1.00     21000\n",
      "   macro avg       1.00      1.00      1.00     21000\n",
      "weighted avg       1.00      1.00      1.00     21000\n",
      "\n",
      "Classification Report(Test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.02      0.03       653\n",
      "           1       0.93      1.00      0.96      8347\n",
      "\n",
      "    accuracy                           0.93      9000\n",
      "   macro avg       0.74      0.51      0.50      9000\n",
      "weighted avg       0.90      0.93      0.89      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report(Train)\")\n",
    "print(classification_report(y_train, y_pred_tr))\n",
    "print(\"Classification Report(Test)\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#! pip install keras\n",
    "#! pip install tensorflow==2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2\n",
      "  Using cached tensorflow-2.2.0-cp38-cp38-win_amd64.whl (459.2 MB)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.4.1)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Using cached tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\vimald\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.2) (1.15.0)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (2.10.0)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (0.34.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (3.13.0)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.32.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Using cached tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\vimald\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.2) (1.12.1)\n",
      "Processing c:\\users\\vimald\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\\termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from tensorflow==2.2) (1.18.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from protobuf>=3.8.0->tensorflow==2.2) (49.2.0.post20200714)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.22.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.24.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\" in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.6.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (19.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.5.1)\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in c:\\users\\vimald\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.7.6)\n",
      "Installing collected packages: google-pasta, tensorflow-estimator, astunparse, keras-preprocessing, absl-py, opt-einsum, grpcio, markdown, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, termcolor, gast, tensorflow\n",
      "Successfully installed absl-py-0.10.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.22.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.2.2 oauthlib-3.1.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.2.2 tensorboard-plugin-wit-1.7.0 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "import pickle\n",
    "filename = 'trained_model_rf_sentiment_analysis.pkl'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-8aeacb087211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGRU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GRU, Dropout, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-3dcb0f9d7e67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_on_texts(final[\"reviews.text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in final[\"reviews.text\"] ])\n",
    "max_legth=max_length #the max length is aroun 1000 character. I would keep it shorter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(t.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(final[\"reviews.text\"], final[\"sentiment\"], test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = t.texts_to_sequences(X_train)\n",
    "X_test = t.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_length, padding = \"post\",truncating = \"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding = \"post\", truncating = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[0:28290]\n",
    "y_train = y_train[0:28290]\n",
    "X_test = X_test[0:9430]\n",
    "y_test = y_test[0:9430]\n",
    "len(y_test),len(X_test),len(X_train),len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils #converting to categorical\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=3)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(units = 32))\n",
    "model.add(Dense(3,activation=\"softmax\")) #since converted to categorical we will have three output nodes. softmax\n",
    "                                         # assigns a probability distribution\n",
    "    \n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, batch_size=10, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x=X_test, y=y_test, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
